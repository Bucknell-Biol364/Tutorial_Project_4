---
title: "Group Project 4"
subtitle: "Biology 368/664 Bucknell University"
output: html_notebook
author: "Abby, Irene, and Sam"
date: 9 Feb 2022
---

## Target Audience
Discuss with your group the target audience for the tutorial. 
Examples could be one of the new core Biology classes, another 300-level course (not 364), or a research group. 

```{r echo=FALSE}

```

## Grading

Each student will be expected to complete the following tasks to earn 85% of the points available for this assignment (21/25).

- Identify and obtain suitable dataset
- Use a Github repository and version control to collaborate on the project
- Spend 4-6 hours preparing, coding, and testing tutorial
  + Data exploration: Sam
  + Data visualization: Irene 
  + Hypothesis testing: Abby 
- Present tutorial in class
- Provide public archive suitable for sharing to students/faculty

Tutorials from previous classes can be viewed at our public github site: https://github.com/Bucknell-Biol364

Each group should use an *Acknowledgements* section to document the participation of each member and the collaboration within and between groups.

Additional credit will be awarded for providing assistance to other groups and for the development of a tutorial that goes beyond the minimal expectations listed above.

#Introducing R

*What is R?* 
R is a programming language that is used to manipulate, visualize, and interpret data. 
R is specifically designed to manipulate large data set and to visualize those data sets in a way that can be used to gather conclusions. It is free to download, making it very accessible to anyone who wants to use it. Currently, R is the industry standard for data science in biology. 

*Introduction to R Studio *
The display screen of R Studio is split into 4 different quadrants, each of which has a different purpose. 

R Script: the upper left window. This is where you will spend the most time in R as it's where you will write your code. 

R Consule: the bottom left window. This is where the program R is actually running. You are also able to write code directly in the console, but there is no way to edit the code you put there after you run it. Generally it's best to write any code in the script so then you have the ability to change it later. 
The console is also where error messages appear. These messages will be usable, in conjunction with other sources as such as Google, in determining the mistakes you made in your code. 

R Environment: the upper right window. This shows a record of all of the objects you currently have saved in R. So any variables, data frames, or variables you have will be shown here. 

R graphical output/files/packages: the bottom right screen. This window has many different functions. Here, you can navigate through your computer and locate files to load into R. The plots tab also allows you to look at any graphs you created. Lastly, the package tab is one way you'll be able to load different packages into R.

*Writing commands in R* 
Writing code is how you'll get anything done in R, so it's best to learn how to do so first. 

The section above said that you write code in the R script window, but the text you're currently reading isn't code. So, in a

*Packages in R*
You can use R just by writing code into the console; however, it is much more useful to use packages. Packages are collections of code, functions, and data sets that make R easier to use. 

Loading in packages 
There are many ways to load packages, but the annoying thing is that a packages must be both installed in computer and then loaded onto your specific document. Even if you already installed a package, you will have to load the same package for every new R markdown you create. 

First is just by using the package tab in the bottom right screen. 

The second method is by writing code that specifically loads the package. This method is preferred; if you were to share your code with anyone, they would be able to use the same packages you installed because you added code to do that. 

Let's do any example of loading in some packages right now. 

Here, we're loading the package "cowplot", which is used to create neater, more uniform looking graphs. 
```{r Load Libraries, include=FALSE}
if (!require("cowplot")) install.packages("cowplot"); library(tidyverse)
```
This code looks complicated, but let's break it down. 
`if (!require("cowplot")) install.packages("cowplot")` tells R to see see if "cowplot" is already installed in your computer. If it's not installed, then it tells R to do so. 
Next, `library(cowplot)` tells R to load cowplot on your current work space. 

To load different packages, change the word "cowplot" in the code above 

<<<<<<< HEAD

=======
>>>>>>> f5c095062c264f4095a26a7037d71eab7f6aa994
#Data exploration 

You can view the data in table form to see if it imported correctly 
*Just type the name of the data set*
```{r summary}

```
Then you can look at the summary of the whole datasheet. This can be done with the summary command. 

*summary(dataset) allows you to look at the summary of the data*
```{r summary 2}

```

The summary command is very powerful in that it provides the basics of the data. It tells you what each variable type (mode) is labled as (character, numeric, integer, complex, or logical). It also provides you with each variables name, number of data points, mean, median, min, max, 1st quartile, and 3rd quartile. 

Sometimes the mode that R picks for the data needs to be changed. 

*new name <- as.character(dataset$variable)* 
*You can check it worked with mode(new name)*
```{r as.character}

```

For this specific variable it would be usefulness to change the mode but sometimes you will run into situations where it is necessary, so always check the class and mode of your variables. 

Now that the whole data set has been seen you can now begin to look at different variables.  

*table(dataset$variable)*
```{r one variable}

```

or you could view tables with 2 variables against each other 

*table(dataset$varible1, dataset$variable2)*
```{r 2 variables}

```

A hypothesis to test for this data is, total sleep depends on body weight. 

For the hypothesis we are looking at body weight so we can look at a box plot of the data. Type the command below.

*ggplot(msleep) + aes(x = bodywt) + geom_histogram() + theme_cowplot()*
```{r normality}

```

From this box plot we can see the normality of the data. It is important that the data be distributed normally because for statistical tests you will see later many assume the data is normal.

We can see here that the data is not distrusted normally, so we can try and transform the to become normally distrusted. 

Try making a histogram of body weight but this time try doing a log transformation. 

*Type the same command as before but this time for x type log(bodywt)*
```{r log transform}

```

The data was made more normal from a log10 transformation. Sometimes the data cannot be transformed to become normal and that is okay. Just make a note when doing a statistical test that the data was not distributed normally.

If you had a more specific hypothesis and wanted to look at a more specific range within the variable you could do that. Maybe you wanted to only look at the brain weights were the body weight was below 100.

*new name <- filter(msleep, (bodywt <= 100))*
*ggplot(new name) + aes(x = log(brainwt)) + geom_histogram() + theme_cowplot()*
```{r filter}

```

If you needed, you could do multiple data filters for different variable at the same time. Just make sure to give the filtered data good name. Sometimes you can end up with a lot of different names, so stay organized. Also you never want to name something the same as the name of the data set because R will overwrite the data with your filtered data. 

When you explore the data enough you can move onto truly visualizing it. 

#Data visulization 

#Hypothesis Testing 
Now that we have looked and explored the data above, you are going to want to check to see if the data is normal. To do this, we will be using a Shapiro-Wilk normality test. First you are going to want to create different plots of the data to look at it individually. The normal Q-Q plot should look linear.  Then we will create the Shapiro test and once you have used this command, you will look at the p-value output. If the p-value is above 0.05 it is normal, and if it is below 0.05 then the data is not normal. 

```{r Data Exploration}
#Testing body weight
simple.eda(msleep$bodywt)
shapiro.test(msleep$bodywt)
```
In this Shapiro-Wilk example, the data is not normal since the p-value of 2.2e-16 is below the 0.05 cutoff. Also, the graph of the Q-Qplot is not linear showing another example that the data is not normal. 

To fix this, you can try to use a log on the data to try to make this more linear. To do this you can create a new vector associated with log of the body weight. Then use the new vector in the same tests as you ran above. 

```{r Fixing Data Exploration}
logbodywt <- log(msleep$bodywt)
simple.eda(logbodywt)
shapiro.test(logbodywt)
```
As you can see, the log transformation was able to make the data more normal. The Q-Q plot shows a more linear graph than it did previously. Also, the p-value with the log transformation is now 0.1048 which is above the 0.05 cutoff, meaning that the data is normal. 

An important note to remember going forward is that you should continue to you the vector "logbodywt" because this uses the transformed data that is now normal rather than old data that was not normal. 

Now, you are going to test your data with a t-test, which is used to statistically tell if there is a difference in means between two different groups. If you have taken a statistics class, you have likely learned about how to do a t-test, but this will do it manually for you. 

```{r}
Carnivore <- dplyr::filter(msleep, vore =="carni")
Herbivore <- dplyr::filter(msleep, vore =="herbi")
```


```{r t-test}
Carni <- logbodywt[msleep$vore == "carni"]
Herbi <- logbodywt[msleep$vore == "herbi"]
t.test(Carni, Herbi)
```



# Acknowledgements

Team Members: 
Sam Whittaker- Data Exploration Section 



